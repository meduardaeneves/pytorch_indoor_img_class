# -*- coding: utf-8 -*-
"""pytorch_indoor_img_class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jj6spffyWsOssE5A6vTQb8d991EpFpRE

Atividade 05 - Maria Eduarda Esteves Neves

# Image Classification using Deep Learning
"""

!nvidia-smi

"""# Dataset

1. Indoor Scene Recognition
2. Classificação de imagens internas com 67 classes
3. Total de 15.620 imagens
4. Pelo menos 100 imagens por classe
5. https://web.mit.edu/torralba/www/indoor.html

### Download dataset

A extração do banco de dados será feita utilizando o wget:
 - wget --> ‘web get’, Utilizando esse comando é feita a extração do banco de dados diretamente da fonte, sem precisar baixá-la para o computador.
"""

!wget http://groups.csail.mit.edu/vision/LabelMe/NewImages/indoorCVPR_09.tar

"""

1.   x: extract an archive.
2.   v: verbose to print its action on console.
3.   f: which file to perform the action on.


"""

#tar vai descompactar as imagens, criando uma pasta dentro de onde você está trabalhando

#ela vem em estrutura onde você tem para cada classe, uma pasta.
#as imagens estão em formato rgb de tamanhos diferentes

!tar -xvf indoorCVPR_09.tar

#mostra onde os arquivos estão salvos, obs: o content é o content da máquina
#virtual;
!pwd

!ls

"""Abaixo apresenta-se um exemplo de uma das imagens existentes no banco de dados."""

import matplotlib.pyplot as plt
import cv2
img = cv2.imread('Images/winecellar/wine_cellar_21_02_altavista.jpg')
plt.imshow(img)

"""### Loading datasets

1. ImageFolder
2. https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html
3. Torchvision Transforms
4. https://pytorch.org/vision/0.15/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py
"""

import torch
from torchvision import transforms, datasets

# Define the data transformation pipeline
train_transform = transforms.Compose([
    #transforms.RandomRotation(30),
    transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(),

    #ToTensor --> joga a informação da imagem dentro de um tensor
    transforms.ToTensor(),
    #normalmente esses dados de normalização vem dos datasets que carregamos para
    #dentro do modelo
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

data_transform = transforms.Compose([
    transforms.Resize((288, 288)),     # Resize the image to (224, 224)
    transforms.ToTensor(),             # Convert image to tensor
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
        ),
    ])

# Specify the root directory of your dataset
data_dir = '/content/Images'

# Create an instance of the ImageFolder dataset
#imagefolder identifica cada pasta como sendo uma classe
train_dataset = datasets.ImageFolder(root=data_dir, transform=train_transform)
test_dataset = datasets.ImageFolder(root=data_dir, transform=data_transform)

dataset_size = len(train_dataset)
indices = list(range(dataset_size))

#Algumas outras possibilidades de transformações

"""
train_transform = transforms.Compose([
    transforms.RandomRotation(30),
    transforms.CenterCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),
    transforms.RandomGrayscale(p=0.2),
    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
"""

"""Albumentations is a computer vision tool that boosts the performance of deep convolutional neural networks.

https://demo.albumentations.ai/
"""

indices[-1]

print(indices)

#a partir do data_set, estou jogando ele para os índices
#class to index, referencia cada uma das classes a um inteiro do índice
test_dataset.class_to_idx

print(test_dataset)

print(test_dataset.class_to_idx)
dicionario_indices = test_dataset.class_to_idx

"""### Dataset splits

1. Using train_test_split from sklearn
2. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
3. Using SubsetRandomSampler from torch
4. https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler
"""

from torch.utils.data.sampler import SubsetRandomSampler
from sklearn.model_selection import train_test_split

train_idx, test_idx = train_test_split(indices, test_size=0.25)
print(len(train_idx), len(test_idx))

# Create a DataLoader to load data in batches
batch_size = 64

train_sampler = SubsetRandomSampler(train_idx)
test_sampler = SubsetRandomSampler(test_idx)

print(train_sampler)
print(test_sampler)

"""Abaixo serão criados os os carregamentos dos dados de teste e treinamento e em seguida duas listas de listas, cada uma delas contendo as classes que foram identificadas nos splits de treinamento e teste para cada uma das diferentes batches de 64 unidades."""

train_dataloader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=batch_size,
    #shuffle=True,
    sampler=train_sampler
    )

test_dataloader = torch.utils.data.DataLoader(
    test_dataset,
    batch_size=batch_size,
    #shuffle=True,
    sampler=test_sampler
    )

# Iterate through the DataLoader
qnt_train = 0
list_classes_train = []

for inputs, labels in train_dataloader:
    # print(inputs.shape, labels)
    # print(labels.min(), labels.max())
    # print('\n')
    lista_tensor = []
    lista_tensor = labels.tolist()
    list_classes_train.append(lista_tensor)
    qnt_train = qnt_train + 1

qnt_test = 0
list_classes_test = []

for inputs, labels in test_dataloader:
    # print(inputs.shape, labels)
    # print(labels.min(), labels.max())
    # print('\n')
    lista_tensor = []
    lista_tensor = labels.tolist()
    list_classes_test.append(lista_tensor)
    qnt_test = qnt_test + 1

print(f'Quantidade de batch de tamanho {batch_size} no Train: {qnt_train}')
print(list_classes_train)
print(f'Quantidade de batch de tamanho {batch_size} no Test: {qnt_test}')
print(list_classes_test)

"""O próximo passo é a conversão de lista de listas em uma única lista, contendo a classe de todos os elementos existentes tanto na etapa de treinamento como teste:"""

lista_completa_train = []
for lista in list_classes_train:
  for elemento in lista:
    lista_completa_train.append(elemento)

lista_ordenada_train = sorted(lista_completa_train)
print('Lista Train:')
print(lista_ordenada_train)
print(f'len Train: {len(lista_ordenada_train)}')

lista_completa_test = []
for lista in list_classes_test:
  for elemento in lista:
    lista_completa_test.append(elemento)

lista_ordenada_test = sorted(lista_completa_test)
print('Lista test:')
print(lista_ordenada_test)
print(f'len test: {len(lista_ordenada_test)}')

lista_indices = []
for indices in dicionario_indices.values():
  lista_indices.append(indices)
print(f'Tamanho da lista de classes: {len(lista_indices)}:\n{lista_indices}')

"""Nesta etapa são criadas duas listas, uma para teste e outra para treinamento, porém, ambas contendo os mesmos elementos:
- Elas serão listas de listas de comprimento 67
- O índice 0 de cada elemento da lista será referente à classe
- o índice 1 de cada elemento da lista referente à quantidade de vezes que aquele elemento foi identificado
"""

qnt_por_classe_train = []
qnt_por_classe_test = []

for indice in lista_indices:
  qnt_elementos_iguais = 0
  for classe in lista_ordenada_train:
    if classe == indice:
      qnt_elementos_iguais += 1
  qnt_por_classe_train.append([indice,qnt_elementos_iguais])

verif_elementos_train = 0
for indice, qnt in qnt_por_classe_train:
  verif_elementos_train += qnt

for indice in lista_indices:
  qnt_elementos_iguais = 0
  for classe in lista_ordenada_test:
    if classe == indice:
      qnt_elementos_iguais += 1
  qnt_por_classe_test.append([indice,qnt_elementos_iguais])

verif_elementos_test = 0
for indice, qnt in qnt_por_classe_test:
  verif_elementos_test += qnt

print(f'Lista de classe do conjunto de treinamento: qnt de imagens = {verif_elementos_train}; qnt de classes = {len(qnt_por_classe_train)}')
print(qnt_por_classe_train)
print()
print(f'Lista de classe do conjunto de teste: qnt de imagens = {verif_elementos_test}; qnt de classes = {len(qnt_por_classe_test)}')
print(qnt_por_classe_test)

"""#### Plotagem da quantidade de elementos por classe depois do SPLIT

Para isso, é feita uma separção dos elementos que representam a quantidade de vezes que a classe aparece e, por fim, é feita uma plotagem da quantidade que cada classe aparece nas etapas de treinamento e teste.
"""

qnt_por_classe_train_y = []
qnt_por_classe_test_y = []

for classe, qnt in qnt_por_classe_train:
  qnt_por_classe_train_y.append(qnt)

for classe, qnt in qnt_por_classe_test:
  qnt_por_classe_test_y.append(qnt)

import plotly.graph_objects as go

fig = go.Figure()
fig.add_trace(go.Bar(
    x=lista_indices,
    y=qnt_por_classe_train_y,
    name='Qnt de imagens por classe - Treinamento',
    marker_color='darkred'
))
fig.add_trace(go.Bar(
    x=lista_indices,
    y=qnt_por_classe_test_y,
    name='Qnt de imagens por classe - Teste',
    marker_color='darkblue'
))

fig.update_layout(
    height=700,
    width=1800,
    #barmode='group',
    xaxis_tickangle=-90
)
fig.show()

"""# Timm

1. Escolheu-se utilizar a biblioteca Timm devido ao fato dela conter diversos modelos pré-treinados
3. https://huggingface.co/docs/timm/index
4. Good review: https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055

"""

import torch

torch.__version__

!pip install timm

import timm

timm.list_models(pretrained=True), len(timm.list_models(pretrained=True))

"""# Training pipeline

De posse dos modelos existentes no Timm, escolheu-se utilizar para o projeto o modelo "resnet18d" devido ao fato do modelo ResNet ser bom para classificação de imagens, ganhando o prêmio de melhor arquitetura na época.
- A arquitetura escolhida foi a ResNet18 ao invés da 50 ou da 100 devido ao fato das demais conterem um grande número de parâmetros e exigirem uma GPU potente e não ser possível rodar somente com o acesso disponibilizado pela google de forma gratuita.
"""

model = timm.create_model('resnet18d', pretrained=True, num_classes=67)

model

x = torch.randn(1, 3, 224, 224)
model(x).shape

model.forward_features(x).shape

model.pretrained_cfg

"""# Training pipeline

## Evaluate
"""

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score
from tqdm.notebook import tqdm

def evaluate(dataloader, model):
  model.cuda()
  model.eval()

  targets = []
  all_preds = []

  for x, y in tqdm(dataloader):
    logits = model(x.cuda())
    pred = logits.argmax(1).flatten().cpu().tolist()

    targets.extend(y.cpu().tolist())
    all_preds.extend(pred)

  acc = accuracy_score(y_true=targets, y_pred=all_preds)
  cm = confusion_matrix(y_true=targets, y_pred=all_preds)
  f1 = f1_score(y_true=targets, y_pred=all_preds, average='weighted')

  return acc, cm, f1

evaluate(test_dataloader, model)

"""## Epoch"""

def epoch(dataloader, model, criterion, optimizer):
  model.cuda()
  model.train()

  epoch_loss = 0.0
  num_batches = 0

  for x, y in tqdm(dataloader):
    # erase the current model gradients
    optimizer.zero_grad()

    # forward
    logits = model(x.cuda())
    loss = criterion(input=logits, target=y.cuda())

    # backward
    loss.backward()

    # weights update ~ w = w_cur - lr*grad(loss, w)
    optimizer.step()

    epoch_loss += loss.detach().cpu().item()
    num_batches += 1

  return epoch_loss/num_batches

"""## Pipeline



1.   Define criterion (https://pytorch.org/docs/stable/nn.html#loss-functions)
2.   Define optimizer (https://pytorch.org/docs/stable/optim.html)
3.   Define hyperparameters (i.e. num epochs, learning rate, ...)
4.   Training loop


"""

summary = {
    'model_state':None,
    'f1_history':[],
    'acc_history':[],
    'cm_history':[],
    'train_loss_history':[]
}

lr = 1e-3
num_epochs = 10
best_acc = 0.0

optimizer = torch.optim.Adam(model.parameters(), lr=lr)
criterion = torch.nn.CrossEntropyLoss(weight=None)


for e in range(num_epochs):
  print('Epoch:', e)
  cur_loss = epoch(train_dataloader, model, criterion, optimizer)
  test_acc, test_cm, test_f1 = evaluate(test_dataloader, model)

  summary['f1_history'].append(test_f1)
  summary['acc_history'].append(test_acc)
  summary['cm_history'].append(test_cm)
  summary['train_loss_history'].append(cur_loss)

  if test_acc > best_acc:
    summary['model_state'] = model.state_dict()
    best_acc = test_acc

  print('\t Training loss:', cur_loss)
  print('\t Test accuracy:', test_acc)
  print('\t Test confusion-matrix:', test_cm)
  print('\t Test F1:', test_f1)
  print()

summary_path = 'summary.pth'

torch.save(summary, summary_path)

summary_path = 'summary.pth'

torch.save(summary, summary_path)

"""# Loading summary

1. Loading and saving torch checkpoint
2. https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html
"""

ckp = torch.load('summary.pth')

"""### Summary keys and values"""

ckp

"""## Resultado

Carregando os melhores resultados obtidos:
- Para isso, vamos utilizar como base a melhor acurária (best_acc)
"""

for indice in range(len(ckp['acc_history'])):
  if ckp['acc_history'][indice] == best_acc:
      print('\t Training loss:', ckp['train_loss_history'][indice])
      print('\t Test accuracy:', ckp['acc_history'][indice])
      print('\t Test F1:', ckp['f1_history'][indice])
      cm_matrix = ckp['cm_history'][indice]
      print('\t Test confusion-matrix:', ckp['cm_history'][indice])

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

fig, ax = plt.subplots(figsize=(25,25))
plt.suptitle("Resnet 18D")
ax.grid(False)

# Exibindo a matriz de confusão diretamente
cmd = ConfusionMatrixDisplay(cm_matrix)

cmd.plot(cmap="Blues", ax=ax)  # Define um colormap para melhor visualização

plt.show()

"""###Conclusão

Observa-se que o modelo testado apresentou uma acurácia maior que 67%. O que apresenta um resultado significativo.
- Em caso de posse de uma GPU melhor e mais potente, seria possível testar diferentes modelos que poderiam apresentar uma acurácia ainda maior que a encontrada neste teste.
"""

# import IPython
# app = IPython.Application.instance()
# app.kernel.do_shutdown(True)